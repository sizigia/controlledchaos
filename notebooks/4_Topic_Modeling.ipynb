{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "from sklearn.decomposition import NMF, LatentDirichletAllocation\n",
    "from sklearn.feature_extraction.text import CountVectorizer, TfidfVectorizer\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn.utils.extmath import randomized_svd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = df.read_csv('../data/lem_stem_text.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "params = {\n",
    "    'vectorizer': {\n",
    "        'analyzer': 'word',\n",
    "        'stop_words': stopwords.words('english'),\n",
    "        'ngram_range': (1, 1),\n",
    "        'token_pattern': '[a-z]{3,}',\n",
    "        'min_df': 0.01,\n",
    "        'lowercase': True\n",
    "    },\n",
    "    'raw_documents': df['stemmed_text'],\n",
    "    'components': 4,\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# CountVectorizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cv = CountVectorizer(**params['vectorizer'])\n",
    "X_cv = cv.fit_transform(raw_documents=params['raw_documents'])\n",
    "\n",
    "cv_doc_word = pd.DataFrame(X_cv.toarray(), columns=cv.get_feature_names())\n",
    "cv_doc_word"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Matrix Decomposition"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "k = 80\n",
    "U, Sigma, VT = randomized_svd(X_cv, \n",
    "                              n_components=k,\n",
    "                              n_iter=5,\n",
    "                              random_state=None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(18, 6))\n",
    "plt.title(\"Sigma vs. K Topics\")\n",
    "plt.ylabel(\"Sigma\")\n",
    "plt.xlabel(\"K Topics\")\n",
    "sns.lineplot(range(k), Sigma)\n",
    "plt.savefig(\"sigma_topics_cv.png\");"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "list(zip(Sigma, range(80)))[:15]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# TF-IDF"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tfidf = TfidfVectorizer(**params['vectorizer'])\n",
    "X_tfidf = tfidf.fit_transform(raw_documents=params['raw_documents'])\n",
    "\n",
    "tfidf_doc_word = pd.DataFrame(X_tfidf.toarray(), columns=tfidf.get_feature_names())\n",
    "tfidf_doc_word"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Matrix Decomposition"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "k = 80\n",
    "U, Sigma, VT = randomized_svd(X_tfidf, \n",
    "                              n_components=k,\n",
    "                              n_iter=5,\n",
    "                              random_state=None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(18, 6))\n",
    "plt.title(\"Sigma vs. K Topics\")\n",
    "plt.ylabel(\"Sigma\")\n",
    "plt.xlabel(\"K Topics\")\n",
    "sns.lineplot(range(k), Sigma)\n",
    "plt.savefig(\"sigma_topics_tfidf.png\");"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pair_sig_topic = list(zip(Sigma, range(k)))\n",
    "for i in range(k):\n",
    "    print(pair_sig_topic[i][0])\n",
    "    if str(pair_sig_topic[i][0])[:3] == str(pair_sig_topic[i + 1][0])[:3]:#(pair_sig_topic[i][0] / pair_sig_topic[i + 1][0]) == 1.0:\n",
    "        print(pair_sig_topic[i][0], pair_sig_topic[i + 1][0])\n",
    "        print(pair_sig_topic[i][1], pair_sig_topic[i + 1][1])\n",
    "        print(pair_sig_topic[i + 2])\n",
    "        print(str(pair_sig_topic[i + 2][0])[:3])\n",
    "        break"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Non-Negative Matrix (NMF)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Use NMF to look for 15 topics\n",
    "n_topics = 15\n",
    "model = NMF(n_components=n_topics)\n",
    "model.fit(X_tfidf)\n",
    "\n",
    "# Print the top 10 words\n",
    "n_words = 10\n",
    "feature_names = tfidf.get_feature_names()\n",
    "\n",
    "topic_list = []\n",
    "for topic_idx, topic in enumerate(model.components_):\n",
    "    top_n = [feature_names[i]\n",
    "             for i in topic.argsort()\n",
    "             [-n_words:]][::-1]\n",
    "    top_features = ' '.join(top_n)\n",
    "    topic_list.append(f\"topic_{'_'.join(top_n[:3])}\") \n",
    "\n",
    "    print(f\"Topic {topic_idx}: {top_features}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Looking for best parameters with GridSearchCV"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "\n",
    "# Beware it will try *all* of the combinations, so it'll take ages\n",
    "search_params = {\n",
    "  'n_components': [5, 10, 15, 20, 25, 30, 40, 50],\n",
    "  'learning_decay': [.2, .5, .7]\n",
    "}\n",
    "\n",
    "# Set up LDA with the options we'll keep static\n",
    "model = LatentDirichletAllocation(learning_method='online')\n",
    "\n",
    "# Try all of the options\n",
    "gridsearch = GridSearchCV(model, param_grid=search_params, n_jobs=-1, verbose=1)\n",
    "gridsearch.fit(X_tfidf)\n",
    "\n",
    "# What did we find?\n",
    "print(\"Best Model's Params: \", gridsearch.best_params_)\n",
    "print(\"Best Log Likelihood Score: \", gridsearch.best_score_)\n",
    "lda_params = gridsearch.best_params_.copy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "\n",
    "# Use LDA to look for 5 topics\n",
    "learning_decay, n_topics = lda_params.values()\n",
    "model = LatentDirichletAllocation(learning_method='online', n_components=n_topics, learning_decay=learning_decay)\n",
    "model.fit(X_tfidf)\n",
    "\n",
    "# Print the top 10 words per topic\n",
    "n_words = 10\n",
    "feature_names = tfidf.get_feature_names()\n",
    "\n",
    "topic_list = []\n",
    "for topic_idx, topic in enumerate(model.components_):\n",
    "    top_n = [feature_names[i]\n",
    "             for i in topic.argsort()\n",
    "             [-n_words:]][::-1]\n",
    "    top_features = ' '.join(top_n)\n",
    "    topic_list.append(f\"topic_{'_'.join(top_n[:3])}\") \n",
    "\n",
    "    print(f\"Topic {topic_idx}: {top_features}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert our counts into numbers\n",
    "amounts = model.transform(X_tfidf) * 100\n",
    "\n",
    "# Set it up as a dataframe\n",
    "topics = pd.DataFrame(amounts, columns=topic_list)\n",
    "topics.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "label = df['stemmed_text'].index\n",
    "doc_topic = pd.DataFrame(model.fit_transform(tfidf.fit_transform(df['stemmed_text'])).round(5),\n",
    "                         index = label,\n",
    "                         columns=['topic{}'.format(i + 1) for i in range(n_topics)])\n",
    "\n",
    "df['top_topic'] = doc_topic.idxmax(axis=1)"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "[e for e in doc_topic.iloc[0].values if (e >= doc_topic.iloc[0].values.max() / 4)]\n",
    "\n",
    "doc_topic.iloc[0].where(doc_topic.iloc[0] > 10)\n",
    "\n",
    "df[['top_topic', 'absolute_path']].groupby(by='top_topic').count()\n",
    "\n",
    "df[['top_topic', 'absolute_path']]"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
